{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9369257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kornia as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "print(AVAIL_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation(nn.Module):\n",
    "    \"\"\"Module to perform data augmentation using Kornia on torch tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, apply_color_jitter: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._apply_color_jitter = apply_color_jitter\n",
    "\n",
    "        self.transforms = nn.Sequential(\n",
    "            K.augmentation.RandomHorizontalFlip(p=0.75),\n",
    "            K.augmentation.RandomChannelShuffle(p=0.75),\n",
    "        )\n",
    "\n",
    "        self.jitter = K.augmentation.ColorJitter(0.5, 0.5, 0.5, 0.5)\n",
    "\n",
    "    @torch.no_grad()  # disable gradients for effiency\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_out = self.transforms(x)  # BxCxHxW\n",
    "        if self._apply_color_jitter:\n",
    "            x_out = self.jitter(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b669471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(nn.Module):\n",
    "    \"\"\"Module to perform pre-process using Kornia on torch tensors.\"\"\"\n",
    "\n",
    "    @torch.no_grad()  # disable gradients for effiency\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x_tmp: np.ndarray = np.array(x)  # HxWxC\n",
    "        x_out: torch.Tensor = K.image_to_tensor(x_tmp, keepdim=True)  # CxHxW\n",
    "        return x_out.float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoolSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CoolSystem, self).__init__()\n",
    "        # not the best model: expereiment yourself\n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        self.preprocess = Preprocess()  # per sample transforms\n",
    "\n",
    "        self.transform = DataAugmentation()  # per batch augmentation_kornia\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x))\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        return F.cross_entropy(y_hat, y)\n",
    "\n",
    "    def show_batch(self, win_size=(10, 10)):\n",
    "\n",
    "        def _to_vis(data):\n",
    "            return K.utils.tensor_to_image(torchvision.utils.make_grid(data, nrow=8))\n",
    "\n",
    "        # get a batch from the training set: try with `val_datlaoader` :)\n",
    "        imgs, labels = next(iter(self.train_dataloader()))\n",
    "        imgs_aug = self.transform(imgs)  # apply transforms\n",
    "        # use matplotlib to visualize\n",
    "        plt.figure(figsize=win_size)\n",
    "        plt.imshow(_to_vis(imgs))\n",
    "        plt.figure(figsize=win_size)\n",
    "        plt.imshow(_to_vis(imgs_aug))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_aug = self.transform(x)  # => we perform GPU/Batched data augmentation\n",
    "        y_hat = self(x_aug)\n",
    "        loss = self.compute_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=False)\n",
    "        self.log(\"train_acc\", self.accuracy(y_hat, y), prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.compute_loss(y_hat, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=False)\n",
    "        self.log(\"valid_acc\", self.accuracy(y_hat, y), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.trainer.max_epochs, 0)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        CIFAR10(os.getcwd(), train=True, download=True, transform=self.preprocess)\n",
    "        CIFAR10(os.getcwd(), train=False, download=True, transform=self.preprocess)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = CIFAR10(os.getcwd(), train=True, download=True, transform=self.preprocess)\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = CIFAR10(os.getcwd(), train=True, download=True, transform=self.preprocess)\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = CoolSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91833cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_batch(win_size=(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21001ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a trainer\n",
    "trainer = Trainer(\n",
    "    progress_bar_refresh_rate=20,\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=3,\n",
    "    logger=pl.loggers.CSVLogger(save_dir='logs/', name=\"cifar10-resnet18\")\n",
    ")\n",
    "\n",
    "# Train the model ⚡\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daacb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\n",
    "print(metrics.head())\n",
    "# 한 epoch 끝에서 valid loss, acc를 계산한다.\n",
    "# 30    6.253247    0.65625      0  1549         NaN        NaN\n",
    "# 31         NaN        NaN      0  1562    6.285731    0.62406\n",
    "\n",
    "\n",
    "aggreg_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggreg_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggreg_metrics)\n",
    "df_metrics[['train_loss', 'valid_loss']].plot(grid=True, legend=True)\n",
    "df_metrics[['valid_acc', 'train_acc']].plot(grid=True, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b974bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
